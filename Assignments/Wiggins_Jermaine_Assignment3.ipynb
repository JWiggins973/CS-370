{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.26 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for numpy==1.26\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# This is to install necessary components to run the assignment\n",
    "# Note: For compatability purposes, libraries have been updated to match to current versions; \n",
    "# hence some of the package invocations may differ slightly from the book\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR_10 is a set of 60K images 32 x 32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "# Constant \n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "# If an UNKNOWN ERROR LOG DISPLAYS, it's just tensorflow trying to connect to GPU. \n",
    "# It'll simply redirect and use the CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot Encoding & Normalization of images\n",
    "# In the book, the older version is:\n",
    "# Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "                        \n",
    "# float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 4,200,842\n",
      "Trainable params: 4,200,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "                input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "# Below red is simply a userwarning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 1.7263 - accuracy: 0.3916 - val_loss: 1.4158 - val_accuracy: 0.5105\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.3730 - accuracy: 0.5130 - val_loss: 1.2785 - val_accuracy: 0.5522\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.2470 - accuracy: 0.5599 - val_loss: 1.2051 - val_accuracy: 0.5739\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 1.1667 - accuracy: 0.5907 - val_loss: 1.1571 - val_accuracy: 0.5884\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.0977 - accuracy: 0.6125 - val_loss: 1.1359 - val_accuracy: 0.6056\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 1.0389 - accuracy: 0.6347 - val_loss: 1.0946 - val_accuracy: 0.6148\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 9s 27ms/step - loss: 0.9978 - accuracy: 0.6514 - val_loss: 1.0739 - val_accuracy: 0.6260\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.9408 - accuracy: 0.6701 - val_loss: 1.0312 - val_accuracy: 0.6415\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.9100 - accuracy: 0.6806 - val_loss: 1.0256 - val_accuracy: 0.6477\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.8681 - accuracy: 0.6968 - val_loss: 1.0220 - val_accuracy: 0.6463\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.8356 - accuracy: 0.7073 - val_loss: 1.0176 - val_accuracy: 0.6569\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.8058 - accuracy: 0.7179 - val_loss: 1.0428 - val_accuracy: 0.6455\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.7766 - accuracy: 0.7280 - val_loss: 1.1296 - val_accuracy: 0.6519\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.7569 - accuracy: 0.7365 - val_loss: 1.0718 - val_accuracy: 0.6566\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.7268 - accuracy: 0.7454 - val_loss: 1.0070 - val_accuracy: 0.6703\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.7065 - accuracy: 0.7528 - val_loss: 1.0520 - val_accuracy: 0.6639\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.6873 - accuracy: 0.7596 - val_loss: 1.0525 - val_accuracy: 0.6564\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.6699 - accuracy: 0.7693 - val_loss: 0.9836 - val_accuracy: 0.6736\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.6451 - accuracy: 0.7775 - val_loss: 1.0610 - val_accuracy: 0.6705\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 8s 27ms/step - loss: 0.6264 - accuracy: 0.7828 - val_loss: 1.1035 - val_accuracy: 0.6667\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 1.1129 - accuracy: 0.6585\n",
      "Test Score: 1.1128840446472168\n",
      "Test Accuracy: 0.6585000157356262\n"
     ]
    }
   ],
   "source": [
    "# Training (NOTE: THIS MAY TAKE SOME TIME. Go grab a cup of coffee!)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIM, metrics=[\"accuracy\"])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print('Test Score:', score[0])\n",
    "print('Test Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "os.makedirs('output', exist_ok=True)\n",
    "model_json = model.to_json()\n",
    "open('./output/cifar10_architecture.json', 'w').write(model_json)\n",
    "\n",
    "# And the weights learned by our deep network on the training set\n",
    "model.save_weights('./output/cifar10.weights.h5', overwrite=True) # NOTE, it is now cifar10.weights, not cifar10_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,168,362\n",
      "Trainable params: 2,168,362\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Improving the CIFAR-10 performance with deeper network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving the CIFAR-10 performance with data augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Note, we are using Tensorflow's Keras package!\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "NUM_TO_AUGMENT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting training set images...\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Augmenting\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "\n",
    "# RUN THE BELOW TO CREATE IMAGES FROM THE ABOVE IMAGEDATAGENERATOR. \n",
    "# NOTE: IF YOU RUN IT, IT WILL TAKE A WHILE!\n",
    "\n",
    "# xtas, ytas = [], []\n",
    "\n",
    "#for i in range(X_train.shape[0]):\n",
    "#    num_aug = 0\n",
    "#     x = X_train[i]  # (32, 32, 3)\n",
    "#     x = x.reshape((1,) + x.shape)  # (1, 32, 32, 3)\n",
    "    \n",
    "#     for x_aug in datagen.flow(x, batch_size=1, save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):\n",
    "#        if num_aug >= NUM_TO_AUGMENT:\n",
    "#             break\n",
    "#         xtas.append(x_aug[0])\n",
    "#         ytas.append(y_train[i])  # save the label as well\n",
    "#         num_aug += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 2.4989 - accuracy: 0.2366 - val_loss: 1.7963 - val_accuracy: 0.3634\n",
      "Epoch 2/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 1.8180 - accuracy: 0.3375 - val_loss: 1.5206 - val_accuracy: 0.4676\n",
      "Epoch 3/20\n",
      "390/390 [==============================] - 34s 87ms/step - loss: 1.7051 - accuracy: 0.3853 - val_loss: 1.4414 - val_accuracy: 0.4692\n",
      "Epoch 4/20\n",
      "390/390 [==============================] - 33s 85ms/step - loss: 1.6393 - accuracy: 0.4120 - val_loss: 1.4541 - val_accuracy: 0.4857\n",
      "Epoch 5/20\n",
      "390/390 [==============================] - 33s 85ms/step - loss: 1.6010 - accuracy: 0.4249 - val_loss: 1.5354 - val_accuracy: 0.4310\n",
      "Epoch 6/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 1.5792 - accuracy: 0.4351 - val_loss: 1.5449 - val_accuracy: 0.4548\n",
      "Epoch 7/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 1.5412 - accuracy: 0.4487 - val_loss: 1.4080 - val_accuracy: 0.4970\n",
      "Epoch 8/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 1.5244 - accuracy: 0.4552 - val_loss: 1.3742 - val_accuracy: 0.5082\n",
      "Epoch 9/20\n",
      "390/390 [==============================] - 34s 87ms/step - loss: 1.5038 - accuracy: 0.4636 - val_loss: 1.4153 - val_accuracy: 0.4943\n",
      "Epoch 10/20\n",
      "390/390 [==============================] - 35s 89ms/step - loss: 1.5004 - accuracy: 0.4641 - val_loss: 1.3125 - val_accuracy: 0.5301\n",
      "Epoch 11/20\n",
      "390/390 [==============================] - 35s 89ms/step - loss: 1.4878 - accuracy: 0.4687 - val_loss: 1.2962 - val_accuracy: 0.5405\n",
      "Epoch 12/20\n",
      "390/390 [==============================] - 35s 89ms/step - loss: 1.4784 - accuracy: 0.4733 - val_loss: 1.2373 - val_accuracy: 0.5700\n",
      "Epoch 13/20\n",
      "390/390 [==============================] - 33s 86ms/step - loss: 1.4745 - accuracy: 0.4762 - val_loss: 1.5507 - val_accuracy: 0.4523\n",
      "Epoch 14/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 1.4696 - accuracy: 0.4773 - val_loss: 1.2909 - val_accuracy: 0.5453\n",
      "Epoch 15/20\n",
      "390/390 [==============================] - 34s 87ms/step - loss: 1.4710 - accuracy: 0.4780 - val_loss: 1.3372 - val_accuracy: 0.5297\n",
      "Epoch 16/20\n",
      "390/390 [==============================] - 34s 88ms/step - loss: 1.4807 - accuracy: 0.4747 - val_loss: 1.4336 - val_accuracy: 0.5098\n",
      "Epoch 17/20\n",
      "390/390 [==============================] - 35s 89ms/step - loss: 1.4688 - accuracy: 0.4783 - val_loss: 1.1871 - val_accuracy: 0.5852\n",
      "Epoch 18/20\n",
      "390/390 [==============================] - 34s 87ms/step - loss: 1.4668 - accuracy: 0.4768 - val_loss: 1.2982 - val_accuracy: 0.5480\n",
      "Epoch 19/20\n",
      "390/390 [==============================] - 34s 87ms/step - loss: 1.4622 - accuracy: 0.4819 - val_loss: 1.2475 - val_accuracy: 0.5604\n",
      "Epoch 20/20\n",
      "390/390 [==============================] - 34s 86ms/step - loss: 1.4599 - accuracy: 0.4844 - val_loss: 1.2095 - val_accuracy: 0.5601\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 1.2095 - accuracy: 0.5601\n",
      "Test Score: 1.2094701528549194\n",
      "Test Accuracy: 0.5601000189781189\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop() # Recreating the optimizer\n",
    "\n",
    "#for the datagen\n",
    "datagen.fit(X_train)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# train changed from model.fit_generator()\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    epochs=NB_EPOCH,\n",
    "    validation_data=tf.data.Dataset.from_tensor_slices((X_test, Y_test)).batch(BATCH_SIZE),\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARKDOWN WORK HERE\n",
    "\n",
    "- Analyze the ethical and privacy implications of the algorithm you just created here!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When working with an image recognition model to tell apart humans from animals or other objects, it is important to make sure the training dataset is diverse. If the dataset is not representative, the model may make mistakes, like misclassifying humans as animals or other objects. This is especially likely if the model is trained mostly on images of one gender, race, or ethnicity. While CIFAR-10 currently classifies animals and vehicles, the same type of model could be used to identify humans, which raises privacy concerns (Wang, Wu, Zhou, & Fu, 2024).<br><br>\n",
    "\n",
    "\n",
    "In a previous course, I learned about Clearview AI, a facial recognition company that collected over 3 billion photos from websites like YouTube, Facebook, and LinkedIn without people’s consent (Hart, 2024). Clearview AI offers a tool to law enforcement that can identify people by matching uploaded images to its database. Similarly, an algorithm like the one used in this assignment could collect and analyze sensitive information without a person’s consent. This could expose personal data and allow people to be identified without permission, showing why it is important to carefully choose training images, protect privacy, and get consent (Wang, Wu, Zhou, & Fu, 2024).<br><br>\n",
    "\n",
    "\n",
    "A dataset that is not diverse can also introduce bias, causing the model to make more errors for certain groups of people. Facial recognition systems have shown this problem, often making more mistakes for women and people of color (Hart, 2024). To reduce these risks, developers should get consent from people, anonymize images, check for bias, and be transparent about how the model works (Wang, Wu, Zhou, & Fu, 2024).<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<b>References</b><br>\n",
    "\n",
    "\n",
    "\n",
    "Hart, R. (2024, September 4). Clearview AI fined yet again for “illegal” Face recognition. Forbes. https://www.forbes.com/sites/roberthart/2024/09/03/clearview-ai-controversial-facial-recognition-firm-fined-33-million-for-illegal-database/ <br>\n",
    "\n",
    "\n",
    "Wang, X., Wu, Y. C., Zhou, M., & Fu, H. (2024). Beyond surveillance: Privacy, ethics, and regulations in face recognition technology. Frontiers in Big Data, 7, 1337465. https://doi.org/10.3389/fdata.2024.1337465<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
