{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.26 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for numpy==1.26\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# This is to install necessary components to run the assignment\n",
    "# Note: For compatability purposes, libraries have been updated to match to current versions; \n",
    "# hence some of the package invocations may differ slightly from the book\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import warnings\n",
    "# Suppress TensorFlow WARNING logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# Suppress Python Future Warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1671) #for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network and Training Variables\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "\n",
    "# Y_train = np_utils.to_categorical() replaced\n",
    "\n",
    "Y_train = to_categorical(y_train, NB_CLASSES) # Updated to fit current version\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 1.5197 - acc: 0.6090 - val_loss: 0.7636 - val_acc: 0.8401\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.6028 - acc: 0.8529 - val_loss: 0.4598 - val_acc: 0.8825\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.4411 - acc: 0.8811 - val_loss: 0.3763 - val_acc: 0.8978\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3807 - acc: 0.8941 - val_loss: 0.3392 - val_acc: 0.9055\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3466 - acc: 0.9027 - val_loss: 0.3134 - val_acc: 0.9103\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3231 - acc: 0.9090 - val_loss: 0.2973 - val_acc: 0.9143\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3049 - acc: 0.9132 - val_loss: 0.2817 - val_acc: 0.9185\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2898 - acc: 0.9176 - val_loss: 0.2704 - val_acc: 0.9218\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2768 - acc: 0.9216 - val_loss: 0.2589 - val_acc: 0.9255\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2652 - acc: 0.9247 - val_loss: 0.2499 - val_acc: 0.9282\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2551 - acc: 0.9272 - val_loss: 0.2411 - val_acc: 0.9302\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2455 - acc: 0.9304 - val_loss: 0.2335 - val_acc: 0.9318\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2370 - acc: 0.9325 - val_loss: 0.2277 - val_acc: 0.9356\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2287 - acc: 0.9355 - val_loss: 0.2202 - val_acc: 0.9366\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2212 - acc: 0.9375 - val_loss: 0.2133 - val_acc: 0.9392\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2140 - acc: 0.9390 - val_loss: 0.2080 - val_acc: 0.9404\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2073 - acc: 0.9412 - val_loss: 0.2027 - val_acc: 0.9438\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2011 - acc: 0.9427 - val_loss: 0.1977 - val_acc: 0.9450\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1953 - acc: 0.9448 - val_loss: 0.1922 - val_acc: 0.9457\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.1897 - acc: 0.9460 - val_loss: 0.1884 - val_acc: 0.9470\n",
      "10000/10000 [==============================] - 0s 15us/sample - loss: 0.1860 - acc: 0.9477\n",
      "Test Score: 0.1859521464794874\n",
      "Test Accuracy: 0.9477\n"
     ]
    }
   ],
   "source": [
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,), activation='relu'))  # First hidden layer\n",
    "model.add(Dense(N_HIDDEN, activation='relu'))                            # Second hidden layer\n",
    "model.add(Dense(NB_CLASSES, activation='softmax'))                       # Output layer\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 8)                 6280      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                90        \n",
      "=================================================================\n",
      "Total params: 6,442\n",
      "Trainable params: 6,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 0s 8us/sample - loss: 2.1633 - acc: 0.2159 - val_loss: 1.9465 - val_acc: 0.3072\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 1.6738 - acc: 0.4645 - val_loss: 1.3836 - val_acc: 0.5695\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 1.2170 - acc: 0.6295 - val_loss: 1.0300 - val_acc: 0.7062\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.9474 - acc: 0.7270 - val_loss: 0.8177 - val_acc: 0.7726\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.7806 - acc: 0.7757 - val_loss: 0.6904 - val_acc: 0.8066\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.6893 - acc: 0.7985 - val_loss: 0.6265 - val_acc: 0.8216\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.6328 - acc: 0.8163 - val_loss: 0.5759 - val_acc: 0.8383\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.5895 - acc: 0.8288 - val_loss: 0.5377 - val_acc: 0.8485\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.5536 - acc: 0.8402 - val_loss: 0.5075 - val_acc: 0.8580\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.5236 - acc: 0.8491 - val_loss: 0.4846 - val_acc: 0.8629\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.5002 - acc: 0.8571 - val_loss: 0.4648 - val_acc: 0.8697\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4806 - acc: 0.8622 - val_loss: 0.4501 - val_acc: 0.8712\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4651 - acc: 0.8673 - val_loss: 0.4376 - val_acc: 0.8753\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4523 - acc: 0.8707 - val_loss: 0.4261 - val_acc: 0.8785\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4412 - acc: 0.8738 - val_loss: 0.4156 - val_acc: 0.8801\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4315 - acc: 0.8768 - val_loss: 0.4093 - val_acc: 0.8835\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4228 - acc: 0.8789 - val_loss: 0.4012 - val_acc: 0.8849\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4152 - acc: 0.8814 - val_loss: 0.3962 - val_acc: 0.8865\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4086 - acc: 0.8821 - val_loss: 0.3904 - val_acc: 0.8872\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 0s 7us/sample - loss: 0.4020 - acc: 0.8841 - val_loss: 0.3837 - val_acc: 0.8912\n",
      "10000/10000 [==============================] - 0s 8us/sample - loss: 0.3882 - acc: 0.8888\n",
      "Test Score: 0.38817953072190287\n",
      "Test Accuracy: 0.8888\n"
     ]
    }
   ],
   "source": [
    "#test 1 N Hidden parameter decreased to 8\n",
    "\n",
    "np.random.seed(1671) #for reproducibility\n",
    "\n",
    "# Network and Training Variables\n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 8 # updated to 8\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "\n",
    "# Y_train = np_utils.to_categorical() replaced\n",
    "\n",
    "Y_train = to_categorical(y_train, NB_CLASSES) # Updated to fit current version\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,), activation='relu'))  # First hidden layer\n",
    "model.add(Dense(N_HIDDEN, activation='relu'))                            # Second hidden layer\n",
    "model.add(Dense(NB_CLASSES, activation='softmax'))                       # Output layer\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 34us/sample - loss: 1.2610 - acc: 0.7141 - val_loss: 0.6167 - val_acc: 0.8629\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.5208 - acc: 0.8703 - val_loss: 0.4100 - val_acc: 0.8933\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.4004 - acc: 0.8917 - val_loss: 0.3433 - val_acc: 0.9069\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.3504 - acc: 0.9022 - val_loss: 0.3122 - val_acc: 0.9132\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.3204 - acc: 0.9094 - val_loss: 0.2892 - val_acc: 0.9212\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2989 - acc: 0.9152 - val_loss: 0.2742 - val_acc: 0.9246\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2820 - acc: 0.9206 - val_loss: 0.2607 - val_acc: 0.9289\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2678 - acc: 0.9245 - val_loss: 0.2504 - val_acc: 0.9305\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 34us/sample - loss: 0.2554 - acc: 0.9283 - val_loss: 0.2398 - val_acc: 0.9343\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2444 - acc: 0.9316 - val_loss: 0.2316 - val_acc: 0.9361\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2347 - acc: 0.9336 - val_loss: 0.2228 - val_acc: 0.9385\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 34us/sample - loss: 0.2253 - acc: 0.9369 - val_loss: 0.2160 - val_acc: 0.9407\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 34us/sample - loss: 0.2172 - acc: 0.9389 - val_loss: 0.2108 - val_acc: 0.9421\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2092 - acc: 0.9417 - val_loss: 0.2032 - val_acc: 0.9429\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.2019 - acc: 0.9435 - val_loss: 0.1968 - val_acc: 0.9448\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.1950 - acc: 0.9452 - val_loss: 0.1924 - val_acc: 0.9473\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.1885 - acc: 0.9470 - val_loss: 0.1870 - val_acc: 0.9492\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.1825 - acc: 0.9490 - val_loss: 0.1820 - val_acc: 0.9509\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.1769 - acc: 0.9503 - val_loss: 0.1772 - val_acc: 0.9513\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 33us/sample - loss: 0.1713 - acc: 0.9517 - val_loss: 0.1737 - val_acc: 0.9521\n",
      "10000/10000 [==============================] - 0s 23us/sample - loss: 0.1714 - acc: 0.9504\n",
      "Test Score: 0.17138539693281055\n",
      "Test Accuracy: 0.9504\n"
     ]
    }
   ],
   "source": [
    "#test 2 N hidden parameter increased to 512\n",
    "\n",
    "np.random.seed(1671) #for reproducibility\n",
    "\n",
    "# Network and Training Variables\n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 512 # update to 512\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "\n",
    "# Y_train = np_utils.to_categorical() replaced\n",
    "\n",
    "Y_train = to_categorical(y_train, NB_CLASSES) # Updated to fit current version\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,), activation='relu'))  # First hidden layer\n",
    "model.add(Dense(N_HIDDEN, activation='relu'))                            # Second hidden layer\n",
    "model.add(Dense(NB_CLASSES, activation='softmax'))                       # Output layer\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,863,690\n",
      "Trainable params: 1,863,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 103us/sample - loss: 1.1700 - acc: 0.7572 - val_loss: 0.5606 - val_acc: 0.8777\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 101us/sample - loss: 0.4819 - acc: 0.8805 - val_loss: 0.3854 - val_acc: 0.8992\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 102us/sample - loss: 0.3775 - acc: 0.8982 - val_loss: 0.3272 - val_acc: 0.9089\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.3328 - acc: 0.9083 - val_loss: 0.2995 - val_acc: 0.9153\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.3054 - acc: 0.9150 - val_loss: 0.2781 - val_acc: 0.9226\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 101us/sample - loss: 0.2855 - acc: 0.9200 - val_loss: 0.2650 - val_acc: 0.9245\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2696 - acc: 0.9244 - val_loss: 0.2515 - val_acc: 0.9290\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 101us/sample - loss: 0.2559 - acc: 0.9281 - val_loss: 0.2424 - val_acc: 0.9311\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2443 - acc: 0.9314 - val_loss: 0.2317 - val_acc: 0.9337\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2338 - acc: 0.9347 - val_loss: 0.2238 - val_acc: 0.9369\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2245 - acc: 0.9368 - val_loss: 0.2152 - val_acc: 0.9390\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2155 - acc: 0.9396 - val_loss: 0.2088 - val_acc: 0.9423\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2077 - acc: 0.9408 - val_loss: 0.2031 - val_acc: 0.9436\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.1999 - acc: 0.9441 - val_loss: 0.1965 - val_acc: 0.9453\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.1930 - acc: 0.9458 - val_loss: 0.1903 - val_acc: 0.9467\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.1864 - acc: 0.9476 - val_loss: 0.1862 - val_acc: 0.9480\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 100us/sample - loss: 0.1800 - acc: 0.9492 - val_loss: 0.1807 - val_acc: 0.9503\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 104us/sample - loss: 0.1742 - acc: 0.9512 - val_loss: 0.1762 - val_acc: 0.9513\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 102us/sample - loss: 0.1688 - acc: 0.9528 - val_loss: 0.1716 - val_acc: 0.9520\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 103us/sample - loss: 0.1634 - acc: 0.9545 - val_loss: 0.1681 - val_acc: 0.9528\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.1652 - acc: 0.9517\n",
      "Test Score: 0.16523353792801498\n",
      "Test Accuracy: 0.9517\n"
     ]
    }
   ],
   "source": [
    "#test 2 N hidden parameter increased to 1024\n",
    "\n",
    "np.random.seed(1671) #for reproducibility\n",
    "\n",
    "# Network and Training Variables\n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 1024 # update to 1024\n",
    "VALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "\n",
    "RESHAPED = 784\n",
    "\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "\n",
    "# Y_train = np_utils.to_categorical() replaced\n",
    "\n",
    "Y_train = to_categorical(y_train, NB_CLASSES) # Updated to fit current version\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,), activation='relu'))  # First hidden layer\n",
    "model.add(Dense(N_HIDDEN, activation='relu'))                            # Second hidden layer\n",
    "model.add(Dense(NB_CLASSES, activation='softmax'))                       # Output layer\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| N_HIDDEN | Test Loss | Test Accuracy |Validation Accuracy \n",
    "|----------|-----------|---------------|-------------------|\n",
    "| 8        | 0.319     | 0.909         | 0.909             |\n",
    "| 128      | 0.233     | 0.933         | 0.934             |\n",
    "| 512      | 0.221     | 0.939         | 0.937             |\n",
    "| 1024     | 0.216     | 0.941         | 0.941             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "I conducted three tests to examine the N_HIDDEN variable at 8, 512, and 1024. The N_HIDDEN variable represents the number of processing neurons in a hidden layer. When decreasing N_HIDDEN to 8, both the test score and accuracy decreased compared to the original setting. The test score represents the categorical crossentropy, or how incorrect the model’s predictions are, while the accuracy indicates how well the model predicts correctly. By examining the data, we see that as N_HIDDEN decreases, the accuracy drops and the test loss increases. The validation accuracy also decreases, which makes sense because fewer neurons per layer mean the model has less capacity to capture patterns in the data.\n",
    "\n",
    "The opposite trend was observed with N_HIDDEN set to 512 and 1024, the test score decreased, and both test accuracy and validation accuracy increased. This demonstrates that increasing the number of neurons allows the model to better learn and generalize from the data. I also noticed that increasing neurons from 128 to 1024 gives only a small improvement because the model already learns most patterns from the training samples, so extra neurons for processing doen't improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
