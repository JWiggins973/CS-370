{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23a25c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.26.0)\n",
      "Requirement already satisfied: keras==3.10.0 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (3.10.0)\n",
      "Requirement already satisfied: tensorflow==2.19 in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.19.0)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (14.1.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (0.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (3.14.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from keras==3.10.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (2.19.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/site-packages (from tensorflow==2.19->-r requirements.txt (line 3)) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.19->-r requirements.txt (line 3)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.19->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.19->-r requirements.txt (line 3)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow==2.19->-r requirements.txt (line 3)) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow==2.19->-r requirements.txt (line 3)) (3.9)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow==2.19->-r requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow==2.19->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/site-packages (from gymnasium->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/site-packages (from gymnasium->-r requirements.txt (line 4)) (8.7.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.9/site-packages (from gymnasium->-r requirements.txt (line 4)) (0.0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.19->-r requirements.txt (line 3)) (0.45.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium->-r requirements.txt (line 4)) (3.23.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.9/site-packages (from rich->keras==3.10.0->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from rich->keras==3.10.0->-r requirements.txt (line 2)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras==3.10.0->-r requirements.txt (line 2)) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a06fb60b-4e81-4bd0-b301-c4517e164e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Average Reward = 45.00, Exploration = 0.990\n",
      "Episode 11: Average Reward = 12.10, Exploration = 0.895\n",
      "Episode 21: Average Reward = 11.90, Exploration = 0.810\n",
      "Episode 31: Average Reward = 8.30, Exploration = 0.732\n",
      "Episode 41: Average Reward = 4.60, Exploration = 0.662\n",
      "Episode 51: Average Reward = 4.50, Exploration = 0.599\n",
      "Episode 61: Average Reward = 22.20, Exploration = 0.542\n",
      "Episode 71: Average Reward = 19.80, Exploration = 0.490\n",
      "Episode 81: Average Reward = 24.00, Exploration = 0.443\n",
      "Episode 91: Average Reward = 54.50, Exploration = 0.401\n",
      "Episode 101: Average Reward = 101.40, Exploration = 0.362\n",
      "Episode 111: Average Reward = 152.80, Exploration = 0.328\n",
      "Episode 121: Average Reward = 145.20, Exploration = 0.296\n",
      "Episode 131: Average Reward = 140.30, Exploration = 0.268\n",
      "Episode 141: Average Reward = 141.40, Exploration = 0.242\n",
      "Episode 151: Average Reward = 138.40, Exploration = 0.219\n",
      "Episode 161: Average Reward = 125.70, Exploration = 0.198\n",
      "Episode 171: Average Reward = 138.60, Exploration = 0.179\n",
      "Episode 181: Average Reward = 129.50, Exploration = 0.162\n",
      "Episode 191: Average Reward = 148.00, Exploration = 0.147\n",
      "Episode 201: Average Reward = 148.50, Exploration = 0.133\n",
      "Episode 211: Average Reward = 144.50, Exploration = 0.120\n",
      "Episode 221: Average Reward = 156.10, Exploration = 0.108\n",
      "Episode 231: Average Reward = 166.40, Exploration = 0.098\n",
      "Episode 241: Average Reward = 167.50, Exploration = 0.089\n",
      "Episode 251: Average Reward = 142.70, Exploration = 0.080\n",
      "Episode 261: Average Reward = 279.60, Exploration = 0.073\n",
      "Episode 271: Average Reward = 197.50, Exploration = 0.066\n",
      "Episode 281: Average Reward = 146.70, Exploration = 0.059\n",
      "Episode 291: Average Reward = 155.60, Exploration = 0.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as Cartpole_model\n"
     ]
    }
   ],
   "source": [
    "# If the assignment takes too long, you may want to download the code \n",
    "# and run it locally to take advantage of your GPU. \n",
    "\n",
    "# Package Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Refactored into Function for reusability\n",
    "def train_cartpole_dqn(\n",
    "    GAMMA=0.99,\n",
    "    EXPLORATION_MAX=1.0,\n",
    "    EXPLORATION_MIN=0.01,\n",
    "    EXPLORATION_DECAY=0.990,\n",
    "    LEARNING_RATE=0.001,\n",
    "    BATCH_SIZE=64,\n",
    "    TRAIN_START=1000,\n",
    "    MEMORY_SIZE=2000,\n",
    "    EPISODES=300\n",
    "):\n",
    "    # Counters during training\n",
    "    train_freq = 4\n",
    "    step_count = 0\n",
    "    target_update_freq = 10\n",
    "    # If an error occurs below, it is because the environment is looking for a GPU.\n",
    "\n",
    "    # Environment setup and Variables\n",
    "    try:\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "    except Exception:\n",
    "        print('Failed to initialize environment! Make sure that gymnasium was installed correctly!')\n",
    "        sys.exit(1)\n",
    "\n",
    "    state_shape = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Initialize memory and rewards\n",
    "    memory = deque(maxlen=MEMORY_SIZE)\n",
    "    episode_rewards = []\n",
    "\n",
    "    # DQN Builder\n",
    "    def build_dqn_model():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(state_shape,)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
    "        return model\n",
    "\n",
    "    model = build_dqn_model()\n",
    "    target_model = build_dqn_model()\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "    # Random Action selection for exploration\n",
    "    def get_action(state, exploration_rate):\n",
    "        if np.random.rand() <= exploration_rate:\n",
    "            return random.randrange(action_size)\n",
    "        q_values = model.predict(np.array([state]), verbose=0) #Remove verbose=0 to see every prediction display\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    # Experience Replay\n",
    "    def experience_replay():\n",
    "        if len(memory) < TRAIN_START:\n",
    "            return\n",
    "        batch = random.sample(memory, BATCH_SIZE)\n",
    "        states = np.zeros((BATCH_SIZE, state_shape))\n",
    "        next_states = np.zeros((BATCH_SIZE, state_shape))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(batch):\n",
    "            states[i] = state\n",
    "            next_states[i] = next_state\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "\n",
    "        target_q = model.predict(states, verbose=0) #Remove verbose=0 to see every prediction display\n",
    "        next_q = target_model.predict(next_states, verbose=0) #Remove verbose=0 to see every prediction display\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                target_q[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target_q[i][actions[i]] = rewards[i] + GAMMA * np.max(next_q[i])\n",
    "\n",
    "        model.fit(states, target_q, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "    # Main loop\n",
    "    # Will iterate through based on the number of EPISODES originally listed up top\n",
    "    for e in range(EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = get_action(state, EXPLORATION_MAX)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            reward = reward if not done else -10\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            step_count += 1\n",
    "            if step_count % train_freq == 0:\n",
    "                experience_replay()\n",
    "\n",
    "        if EXPLORATION_MAX > EXPLORATION_MIN:\n",
    "            EXPLORATION_MAX *= EXPLORATION_DECAY\n",
    "            EXPLORATION_MAX = max(EXPLORATION_MIN, EXPLORATION_MAX)\n",
    "\n",
    "        if e % target_update_freq == 0:\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        # This will display the average reward of every 10 episodes occurring.\n",
    "        # Comment this section and add the below print method if you want to display every episode\n",
    "        if e % 10 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-10:])\n",
    "            print(f\"Episode {e+1}: Average Reward = {avg_reward:.2f}, Exploration = {EXPLORATION_MAX:.3f}\")\n",
    "\n",
    "        # Logic to complete execution if average is greater than 195\n",
    "        if len(episode_rewards) >= 100:\n",
    "            avg_last_hun = np.mean(episode_rewards[-100:])\n",
    "            if avg_last_hun >= 195:\n",
    "                print(f\"Solved at episode {e}: Average reward over last 100: {avg_last_hun:.2f}\")\n",
    "                break\n",
    "\n",
    "    # Saving model as needed    \n",
    "    model.save(\"Cartpole_model.h5\")\n",
    "    print(\"Training complete! Model saved as Cartpole_model\")\n",
    "\n",
    "    # Note, if an error displays, it is attempting to connect to the GPU.\n",
    "    # It will not connect and run on CPU. \n",
    "    # Code will take some time to run, which is commonplace for real life models.\n",
    "    # Make sure your computer does not go to sleep, and take a well-deserved break!\n",
    "    \n",
    "    # Return statement for reusability\n",
    "    return model, episode_rewards\n",
    "\n",
    "baseline, rewards1 = train_cartpole_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01640-e7be-4b19-8e09-c8e84496c812",
   "metadata": {},
   "source": [
    "# Explain how reinforcement learning concepts apply to the cartpole problem\n",
    "Reinforcement learning is learning through interactions, analyzing the results, and adjusting its moves to maximize the rewards (Simonini, 2018). In the CartPole problem, the goal of the agent is to balance the pole upright on a moving cart for as long as possible. The action the agent makes is based on the current state variables, which describe how the environment looks at the moment. In this problem, these are the cart’s position, the cart’s current velocity, the angle of the pole, and the speed at which the pole is falling. The possible actions for the agent are to move left or right, and it receives rewards if the pole is balanced. To learn the best way to balance the pole, a Deep Q-Network (DQN) algorithm is used, which is a neural network that learns which actions are best in each state by predicting future rewards (Simonini, 2018).\n",
    "\n",
    "# Analyze how experience replay is applied to the cartpole problem\n",
    "Experience replay is a technique used in reinforcement learning algorithms that remembers past experiences and reuses them for training (Beysolow, 2019). In the CartPole problem, each time the agent moves the cart left or right, it saves the experience (state, action, reward, next_state, done) in memory. During training, the agent randomly picks experiences from memory to train its neural network. This helps the agent learn from many different situations, avoid depending only on recent moves, and improve its ability to keep the pole balanced longer.\n",
    "\n",
    "The discount factor determines how much the agent values future rewards versus immediate rewards. Introducing this variable into the algorithm allows the agent to balance short-term gains with long-term goals, helping it plan actions that maximize total reward over time (Beysolow, 2019).\n",
    "\n",
    "# Analyze how neural networks are used in deep Q-learning.\n",
    "In this problem, the neural network has an input layer that takes the current state as input, processes this through two hidden layers with 64 neurons each, and outputs Q-values for each possible action. The agent then chooses the action that corresponds to the higher Q-value because it predicts a greater reward. Using a neural network also makes the Q-learning algorithm more efficient, because it can generalize from past experiences and predict Q-values for states it hasn’t seen before, instead of needing a table of every possible state and action (Beysolow, 2019). The learning rate controls how quickly the network updates its predictions. A high learning rate can make the agent learn faster but may be unstable, while a low learning rate makes learning slower but more stable. As seen below\n",
    "\n",
    "# References\n",
    "Beysolow, I. T. (2019). Applied reinforcement learning with python : With openai gym, tensorflow, and keras. Apress L. P..\n",
    "\n",
    "Simonini, T. (2018, September 3). An introduction to Q-Learning: reinforcement learning. freeCodeCamp. https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff09d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Average Reward = 8.00, Exploration = 0.100\n",
      "Episode 11: Average Reward = -1.90, Exploration = 0.010\n",
      "Episode 21: Average Reward = -1.30, Exploration = 0.010\n",
      "Episode 31: Average Reward = 0.70, Exploration = 0.010\n",
      "Episode 41: Average Reward = -1.70, Exploration = 0.010\n",
      "Episode 51: Average Reward = -0.80, Exploration = 0.010\n",
      "Episode 61: Average Reward = -1.30, Exploration = 0.010\n",
      "Episode 71: Average Reward = -0.50, Exploration = 0.010\n",
      "Episode 81: Average Reward = -1.20, Exploration = 0.010\n",
      "Episode 91: Average Reward = 0.20, Exploration = 0.010\n",
      "Episode 101: Average Reward = 10.10, Exploration = 0.010\n",
      "Episode 111: Average Reward = 8.20, Exploration = 0.010\n",
      "Episode 121: Average Reward = 3.70, Exploration = 0.010\n",
      "Episode 131: Average Reward = 2.80, Exploration = 0.010\n",
      "Episode 141: Average Reward = 4.00, Exploration = 0.010\n",
      "Episode 151: Average Reward = 3.80, Exploration = 0.010\n",
      "Episode 161: Average Reward = 7.20, Exploration = 0.010\n",
      "Episode 171: Average Reward = 28.70, Exploration = 0.010\n",
      "Episode 181: Average Reward = 33.40, Exploration = 0.010\n",
      "Episode 191: Average Reward = 70.90, Exploration = 0.010\n",
      "Episode 201: Average Reward = 104.90, Exploration = 0.010\n",
      "Episode 211: Average Reward = 174.40, Exploration = 0.010\n",
      "Episode 221: Average Reward = 131.70, Exploration = 0.010\n",
      "Episode 231: Average Reward = 167.60, Exploration = 0.010\n",
      "Episode 241: Average Reward = 139.20, Exploration = 0.010\n",
      "Episode 251: Average Reward = 150.60, Exploration = 0.010\n",
      "Episode 261: Average Reward = 133.80, Exploration = 0.010\n",
      "Episode 271: Average Reward = 118.70, Exploration = 0.010\n",
      "Episode 281: Average Reward = 112.70, Exploration = 0.010\n",
      "Episode 291: Average Reward = 97.10, Exploration = 0.010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as Cartpole_model\n",
      "Episode 1: Average Reward = 12.00, Exploration = 0.999\n",
      "Episode 11: Average Reward = 7.20, Exploration = 0.989\n",
      "Episode 21: Average Reward = 19.70, Exploration = 0.979\n",
      "Episode 31: Average Reward = 7.70, Exploration = 0.969\n",
      "Episode 41: Average Reward = 18.10, Exploration = 0.960\n",
      "Episode 51: Average Reward = 14.70, Exploration = 0.950\n",
      "Episode 61: Average Reward = 17.40, Exploration = 0.941\n",
      "Episode 71: Average Reward = 12.10, Exploration = 0.931\n",
      "Episode 81: Average Reward = 11.70, Exploration = 0.922\n",
      "Episode 91: Average Reward = 8.50, Exploration = 0.913\n",
      "Episode 101: Average Reward = 9.80, Exploration = 0.904\n",
      "Episode 111: Average Reward = 18.50, Exploration = 0.895\n",
      "Episode 121: Average Reward = 22.10, Exploration = 0.886\n",
      "Episode 131: Average Reward = 24.10, Exploration = 0.877\n",
      "Episode 141: Average Reward = 22.10, Exploration = 0.868\n",
      "Episode 151: Average Reward = 19.20, Exploration = 0.860\n",
      "Episode 161: Average Reward = 14.40, Exploration = 0.851\n",
      "Episode 171: Average Reward = 21.50, Exploration = 0.843\n",
      "Episode 181: Average Reward = 24.00, Exploration = 0.834\n",
      "Episode 191: Average Reward = 26.50, Exploration = 0.826\n",
      "Episode 201: Average Reward = 23.00, Exploration = 0.818\n",
      "Episode 211: Average Reward = 28.00, Exploration = 0.810\n",
      "Episode 221: Average Reward = 18.40, Exploration = 0.802\n",
      "Episode 231: Average Reward = 18.90, Exploration = 0.794\n",
      "Episode 241: Average Reward = 21.20, Exploration = 0.786\n",
      "Episode 251: Average Reward = 25.40, Exploration = 0.778\n",
      "Episode 261: Average Reward = 24.60, Exploration = 0.770\n",
      "Episode 271: Average Reward = 22.50, Exploration = 0.763\n",
      "Episode 281: Average Reward = 29.90, Exploration = 0.755\n",
      "Episode 291: Average Reward = 32.40, Exploration = 0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as Cartpole_model\n"
     ]
    }
   ],
   "source": [
    "# update exploration factor\n",
    "explorationTest1, rewards1 = train_cartpole_dqn(EXPLORATION_DECAY = 0.100) # test low\n",
    "\n",
    "explorationTest2, rewards2 = train_cartpole_dqn(EXPLORATION_DECAY = 0.999) # test high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6eb24-c628-471f-af4f-4f7c4851f465",
   "metadata": {},
   "source": [
    "# Exploration observation\n",
    "With low exploration (0.1), the agent eventually learns a good strategy and gets very high rewards. With high exploration (0.999), the agent keeps trying many random actions, so rewards stay consistent and never reach the same high values. This shows that too much exploration can prevent the agent from maximizing its performance. Neither test get an average reward of 195 over 100 games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42cb9c50-5fab-43b5-8298-c907ff897b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Average Reward = 26.00, Exploration = 0.990\n",
      "Episode 11: Average Reward = 6.90, Exploration = 0.895\n",
      "Episode 21: Average Reward = 18.00, Exploration = 0.810\n",
      "Episode 31: Average Reward = 5.00, Exploration = 0.732\n",
      "Episode 41: Average Reward = 4.60, Exploration = 0.662\n",
      "Episode 51: Average Reward = 8.20, Exploration = 0.599\n",
      "Episode 61: Average Reward = 5.80, Exploration = 0.542\n",
      "Episode 71: Average Reward = 2.60, Exploration = 0.490\n",
      "Episode 81: Average Reward = 1.70, Exploration = 0.443\n",
      "Episode 91: Average Reward = -0.10, Exploration = 0.401\n",
      "Episode 101: Average Reward = 0.30, Exploration = 0.362\n",
      "Episode 111: Average Reward = 1.00, Exploration = 0.328\n",
      "Episode 121: Average Reward = -0.70, Exploration = 0.296\n",
      "Episode 131: Average Reward = 8.20, Exploration = 0.268\n",
      "Episode 141: Average Reward = 9.00, Exploration = 0.242\n",
      "Episode 151: Average Reward = 15.00, Exploration = 0.219\n",
      "Episode 161: Average Reward = 6.10, Exploration = 0.198\n",
      "Episode 171: Average Reward = 15.60, Exploration = 0.179\n",
      "Episode 181: Average Reward = 25.30, Exploration = 0.162\n",
      "Episode 191: Average Reward = 29.80, Exploration = 0.147\n",
      "Episode 201: Average Reward = 20.00, Exploration = 0.133\n",
      "Episode 211: Average Reward = 3.60, Exploration = 0.120\n",
      "Episode 221: Average Reward = 27.10, Exploration = 0.108\n",
      "Episode 231: Average Reward = 15.40, Exploration = 0.098\n",
      "Episode 241: Average Reward = 10.60, Exploration = 0.089\n",
      "Episode 251: Average Reward = -1.00, Exploration = 0.080\n",
      "Episode 261: Average Reward = 11.90, Exploration = 0.073\n",
      "Episode 271: Average Reward = 9.60, Exploration = 0.066\n",
      "Episode 281: Average Reward = 9.20, Exploration = 0.059\n",
      "Episode 291: Average Reward = 11.50, Exploration = 0.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as Cartpole_model\n",
      "Episode 1: Average Reward = 18.00, Exploration = 0.990\n",
      "Episode 11: Average Reward = 10.60, Exploration = 0.895\n",
      "Episode 21: Average Reward = 5.00, Exploration = 0.810\n",
      "Episode 31: Average Reward = 8.30, Exploration = 0.732\n",
      "Episode 41: Average Reward = 3.20, Exploration = 0.662\n",
      "Episode 51: Average Reward = 5.30, Exploration = 0.599\n",
      "Episode 61: Average Reward = 1.70, Exploration = 0.542\n",
      "Episode 71: Average Reward = 0.60, Exploration = 0.490\n",
      "Episode 81: Average Reward = 3.00, Exploration = 0.443\n",
      "Episode 91: Average Reward = 12.60, Exploration = 0.401\n",
      "Episode 101: Average Reward = 12.70, Exploration = 0.362\n",
      "Episode 111: Average Reward = 28.70, Exploration = 0.328\n",
      "Episode 121: Average Reward = 54.50, Exploration = 0.296\n",
      "Episode 131: Average Reward = 102.90, Exploration = 0.268\n",
      "Episode 141: Average Reward = 349.80, Exploration = 0.242\n",
      "Episode 151: Average Reward = 295.50, Exploration = 0.219\n",
      "Episode 161: Average Reward = 430.50, Exploration = 0.198\n",
      "Episode 171: Average Reward = 401.90, Exploration = 0.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved at episode 178: Average reward over last 100: 196.08\n",
      "Training complete! Model saved as Cartpole_model\n"
     ]
    }
   ],
   "source": [
    "#update gamma\n",
    "gammaTest, rewards1 = train_cartpole_dqn(GAMMA=0.0) # test low\n",
    "\n",
    "gammaTest2, rewards2 = train_cartpole_dqn(GAMMA=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839d38b-1284-447f-aedc-792a517f02f8",
   "metadata": {},
   "source": [
    "# Gamma observation\n",
    "With low gamma (0), the agent only considers immediate rewards, so it fails to plan for keeping the pole balanced over time. As a result, average rewards remain low and inconsistent. With high gamma (0.9), the agent accounts for future rewards and learned strategies that kept the pole upright longer. This allowed the agent to solve CartPole by episode 178, achieving an average reward of 196 over the last 100 episodes, something it could not do with the lower gamma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0490cf6-3688-4d92-a5f1-f13f931c9a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Average Reward = 14.00, Exploration = 0.990\n",
      "Episode 11: Average Reward = 15.30, Exploration = 0.895\n",
      "Episode 21: Average Reward = 14.00, Exploration = 0.810\n",
      "Episode 31: Average Reward = 7.50, Exploration = 0.732\n",
      "Episode 41: Average Reward = 2.40, Exploration = 0.662\n",
      "Episode 51: Average Reward = 4.00, Exploration = 0.599\n",
      "Episode 61: Average Reward = 2.30, Exploration = 0.542\n",
      "Episode 71: Average Reward = 8.00, Exploration = 0.490\n",
      "Episode 81: Average Reward = 9.40, Exploration = 0.443\n",
      "Episode 91: Average Reward = 5.60, Exploration = 0.401\n",
      "Episode 101: Average Reward = 8.60, Exploration = 0.362\n",
      "Episode 111: Average Reward = 15.40, Exploration = 0.328\n",
      "Episode 121: Average Reward = 12.20, Exploration = 0.296\n",
      "Episode 131: Average Reward = 5.20, Exploration = 0.268\n",
      "Episode 141: Average Reward = 6.00, Exploration = 0.242\n",
      "Episode 151: Average Reward = 6.30, Exploration = 0.219\n",
      "Episode 161: Average Reward = 5.10, Exploration = 0.198\n",
      "Episode 171: Average Reward = 5.00, Exploration = 0.179\n",
      "Episode 181: Average Reward = 3.40, Exploration = 0.162\n",
      "Episode 191: Average Reward = 2.40, Exploration = 0.147\n",
      "Episode 201: Average Reward = 1.10, Exploration = 0.133\n",
      "Episode 211: Average Reward = -0.30, Exploration = 0.120\n",
      "Episode 221: Average Reward = 0.00, Exploration = 0.108\n",
      "Episode 231: Average Reward = -1.10, Exploration = 0.098\n",
      "Episode 241: Average Reward = -1.10, Exploration = 0.089\n",
      "Episode 251: Average Reward = -1.30, Exploration = 0.080\n",
      "Episode 261: Average Reward = -1.70, Exploration = 0.073\n",
      "Episode 271: Average Reward = -1.10, Exploration = 0.066\n",
      "Episode 281: Average Reward = -1.10, Exploration = 0.059\n",
      "Episode 291: Average Reward = -0.70, Exploration = 0.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as Cartpole_model\n",
      "Episode 1: Average Reward = 1.00, Exploration = 0.990\n",
      "Episode 11: Average Reward = 12.90, Exploration = 0.895\n",
      "Episode 21: Average Reward = 16.10, Exploration = 0.810\n",
      "Episode 31: Average Reward = 14.30, Exploration = 0.732\n",
      "Episode 41: Average Reward = 13.90, Exploration = 0.662\n",
      "Episode 51: Average Reward = 9.20, Exploration = 0.599\n",
      "Episode 61: Average Reward = 8.50, Exploration = 0.542\n",
      "Episode 71: Average Reward = 14.90, Exploration = 0.490\n",
      "Episode 81: Average Reward = 8.50, Exploration = 0.443\n",
      "Episode 91: Average Reward = 6.00, Exploration = 0.401\n",
      "Episode 101: Average Reward = 18.60, Exploration = 0.362\n",
      "Episode 111: Average Reward = 33.00, Exploration = 0.328\n",
      "Episode 121: Average Reward = 59.20, Exploration = 0.296\n",
      "Episode 131: Average Reward = 33.30, Exploration = 0.268\n",
      "Episode 141: Average Reward = 35.40, Exploration = 0.242\n",
      "Episode 151: Average Reward = 42.30, Exploration = 0.219\n",
      "Episode 161: Average Reward = 85.10, Exploration = 0.198\n",
      "Episode 171: Average Reward = 83.20, Exploration = 0.179\n",
      "Episode 181: Average Reward = 27.10, Exploration = 0.162\n",
      "Episode 191: Average Reward = 32.30, Exploration = 0.147\n",
      "Episode 201: Average Reward = 70.50, Exploration = 0.133\n",
      "Episode 211: Average Reward = 64.10, Exploration = 0.120\n",
      "Episode 221: Average Reward = 47.40, Exploration = 0.108\n",
      "Episode 231: Average Reward = 64.90, Exploration = 0.098\n",
      "Episode 241: Average Reward = 91.00, Exploration = 0.089\n",
      "Episode 251: Average Reward = 58.00, Exploration = 0.080\n",
      "Episode 261: Average Reward = 89.10, Exploration = 0.073\n",
      "Episode 271: Average Reward = 24.10, Exploration = 0.066\n",
      "Episode 281: Average Reward = 54.10, Exploration = 0.059\n",
      "Episode 291: Average Reward = 41.80, Exploration = 0.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved as Cartpole_model\n"
     ]
    }
   ],
   "source": [
    "#update learning rate\n",
    "learningRate, rewards1 = train_cartpole_dqn(LEARNING_RATE=0.0001) # test low\n",
    "\n",
    "learningRate2, rewards1 = train_cartpole_dqn(LEARNING_RATE=0.1) # test high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e035d199-7c38-45a2-9e64-769d0b0d15b4",
   "metadata": {},
   "source": [
    "# Learning rate observation\n",
    "With a very low learning rate (0.0001), the agent learns very slowly, and average rewards remain low or even decline over time, showing it struggles to solve the problem. With a high learning rate (0.1), the agent learns much faster and achieves higher rewards, but its performance is inconsistent. In neither case was the agent able to achieve an average score of 195 over 100 episodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
